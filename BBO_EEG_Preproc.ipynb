{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import some libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import h5io\n",
    "from pyprep.prep_pipeline import PrepPipeline\n",
    "import pyprep as ppr\n",
    "from Path_Config import Raw_Data_Path as data_path\n",
    "from BBO_Analysis_Functions import infer_rights, correct_drift\n",
    "# matplotlib.use('Qt5Agg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Set of cells are all for ICA cleaning and channel selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the figures of the presentation tomorrow\n",
    "#%%############################################################################\n",
    "\"\"\"import data\"\"\"\n",
    "\"\"\"This first part is only for computing the ICA on high pass filtered data and Channel selection\"\"\"\n",
    "###############################################################################\n",
    "for sub in range(26,30):\n",
    "    index = sub\n",
    "    Chan_sel = False\n",
    "    working_dir = data_path + \"EEG/\" \n",
    "    EEGfiles = [f for f in os.listdir(working_dir) if f.endswith('.vhdr')]\n",
    "    file = EEGfiles[index]\n",
    "    print(f'Working on file {file}')\n",
    "    EEG_fpath = os.path.join(working_dir, file)\n",
    "    data = mne.io.read_raw_brainvision(EEG_fpath, preload = True)\n",
    "\n",
    "    # Filter data\n",
    "    data.load_data().filter(l_freq = .3, h_freq = 35)\n",
    "    data.set_eeg_reference(ref_channels = \"average\")\n",
    "\n",
    "    # Extract and label events\n",
    "    events = mne.events_from_annotations(data)\n",
    "    events = events[0]\n",
    "    event_id = {'Left':10002,'Invalid':10003, 'Right':10004}\n",
    "\n",
    "    # Remove invalid events\n",
    "    event_dict = {}\n",
    "    eventcount = {}\n",
    "    for key, val in event_id.items():\n",
    "        numevents = len(events[events[:,2] == val])\n",
    "        temp = events[events[:,2] == val]\n",
    "        print(f'The original number of events for {key} is {numevents}')\n",
    "        eventcount[key] = numevents\n",
    "        event_dict[key] = temp\n",
    "        if numevents > 45:\n",
    "            temp = temp[-45:,:]\n",
    "            event_dict[key] = temp\n",
    "        elif numevents == 0:\n",
    "            temp = []\n",
    "            event_dict[key] = temp\n",
    "        elif numevents == 12:\n",
    "            event_dict[key] = temp\n",
    "\n",
    "\n",
    "    # Import the csv file with psychopy data\n",
    "    responsepath = data_path + \"Psychopy/\"\n",
    "    # extract all .csv files in the folder\n",
    "    responsefiles = [f for f in os.listdir(responsepath) if f.endswith('.csv') and (f.startswith(str(index+1)+'_') and not f.__contains__('trial'))]\n",
    "    # # sort in ascending order\n",
    "    responsefiles.sort()\n",
    "    filename = responsefiles[0]\n",
    "    print(filename)\n",
    "\n",
    "    # try each file to see if it is the correct one\n",
    "    for filename in responsefiles:\n",
    "        fpath = os.path.join(responsepath, filename)\n",
    "        responses = pd.read_csv(fpath)\n",
    "        if 'p_port' not in responses.keys():\n",
    "            continue\n",
    "        elif len(responses['p_port']>1) == 102:\n",
    "            break\n",
    "        \n",
    "    # Assert that the EEG and the response file match in the participant number\n",
    "    assert int(filename.split('_')[0]) == index+1 == int(file.split('_')[1]), 'EEG and Psychopy files dont match'\n",
    "\n",
    "    fpath = os.path.join(responsepath, filename)\n",
    "    responses = pd.read_csv(fpath)\n",
    "\n",
    "    # If there are no right events, infer the right indices and correct for drift\n",
    "    if len(event_dict['Right']) == 0:\n",
    "        print('No right events found. Inferring and correcting for drift...')\n",
    "        srate = data.info['sfreq']\n",
    "        left_idx = event_dict['Left'][:,0]\n",
    "        right_idx = infer_rights(responses, srate, left_idx)\n",
    "        right_idx = correct_drift(responses, srate, left_idx, right_idx)\n",
    "        event_dict['Right'] = np.zeros((len(right_idx),3))\n",
    "        event_dict['Right'][:,0] = right_idx\n",
    "        event_dict['Right'][:,1] = 0\n",
    "        event_dict['Right'][:,2] = 10004\n",
    "\n",
    "    # Save the events in the EEG file\n",
    "    events_in_eeg = np.load(os.path.join(working_dir,'events_in_eeg.npy'), allow_pickle=True).item()\n",
    "    events_in_eeg[file] = eventcount # This will not work\n",
    "    np.save(os.path.join(working_dir,'events_in_eeg.npy'), events_in_eeg) \n",
    "\n",
    "    # Now convert the dict back to an array\n",
    "    events = np.concatenate([v for v in event_dict.values()], axis=0)\n",
    "    del event_dict\n",
    "    events = events.astype(int)\n",
    "    events = events[events[:,0].argsort()]\n",
    "\n",
    "    assert len(events[:,2][events[:,2] == 10002]) == 45 == len(events[:,2][events[:,2] == 10004]), 'Events are not 45'\n",
    "    assert len(events[:,2][events[:,2] == 10003]) == 12, 'Invalid events are not 12'\n",
    "\n",
    "    # Set Montage\n",
    "    montage = mne.channels.make_standard_montage('easycap-M1')\n",
    "    data.rename_channels({'M1': 'TP9', 'M2': 'TP10'})\n",
    "    data.set_montage(montage)\n",
    "\n",
    "    # #%%############################################################################\n",
    "    # \"\"\"remove and interpolate bad channels\"\"\"\n",
    "    # ###############################################################################\n",
    "\n",
    "    # Instantiate NoisyChannels with Raw data\n",
    "    noisy_chans = ppr.NoisyChannels(data, do_detrend=False)\n",
    "\n",
    "    # Find bad by correlation\n",
    "    noisy_chans.find_bad_by_correlation(correlation_secs=5.0, correlation_threshold=0.4, frac_bad=0.1)\n",
    "    print(f'After correlation removal: {noisy_chans.get_bads()}')\n",
    "    data.info['bads'].extend(noisy_chans.get_bads())  # Update bads\n",
    "\n",
    "    # Find bad by deviation\n",
    "    noisy_chans.find_bad_by_deviation()\n",
    "    print(f'After deviation removal: {noisy_chans.bad_by_deviation}')\n",
    "    data.info['bads'].extend(noisy_chans.get_bads())  # Update bads\n",
    "\n",
    "    # Find bad by high-frequency noise\n",
    "    noisy_chans.find_bad_by_hfnoise(HF_zscore_threshold=5.0)\n",
    "    print(f'After HF noise removal: {noisy_chans.get_bads()}')\n",
    "    data.info['bads'].extend(noisy_chans.get_bads())  # Update bads\n",
    "    data.info['bads'] = list(set(data.info['bads']))\n",
    "    bad_channels_ERPs = list(set(data.info['bads']))\n",
    "    data.interpolate_bads(reset_bads=True)\n",
    "\n",
    "    # # Run RANSAC to find additional bad channels\n",
    "    # noisy_chans.find_bad_by_ransac(n_samples=50, \n",
    "    #                             sample_prop=0.25, \n",
    "    #                             corr_thresh=0.75, \n",
    "    #                             frac_bad=0.4, \n",
    "    #                             corr_window_secs=5.0, \n",
    "    #                             channel_wise=False, \n",
    "    #                             max_chunk_size=None)\n",
    "    # print(f'After RANSAC removal: {noisy_chans.get_bads()}')\n",
    "    # print(f'Bad channels before RANSAC: {data.info[\"bads\"]}')\n",
    "    # data.info['bads'].extend(noisy_chans.get_bads())  # Update bads\n",
    "    # data.info['bads'] = list(set(data.info['bads']))\n",
    "    # bad_channels_ERPs = list(set(data.info['bads']))\n",
    "    # data.interpolate_bads(reset_bads=True)\n",
    "\n",
    "    #%%############################################################################\n",
    "    \"\"\"Epoch data\"\"\"\n",
    "    ###############################################################################\n",
    "    epstart = -.5\n",
    "    epend = 1.5\n",
    "    Selection_epochs = mne.Epochs(data,\n",
    "                        events = events,\n",
    "                        event_id = event_id,\n",
    "                        tmin = epstart, tmax = epend,\n",
    "                        proj = False, baseline = None,\n",
    "                        preload = True, reject = None)\n",
    "    Selection_epochs.set_eeg_reference(ref_channels = \"average\")\n",
    "\n",
    "    #%%############################################################################\n",
    "    \"\"\"Independent Components Analysis\"\"\"\n",
    "    ###############################################################################\n",
    "\n",
    "    ncomp = 30\n",
    "    ica = mne.preprocessing.ICA(n_components = ncomp)\n",
    "    ica.fit(Selection_epochs)\n",
    "    # ica.plot_properties(Selection_epochs, picks=range(0, 20))\n",
    "    # Look at the timecourse of the component\n",
    "    ica.plot_sources(Selection_epochs)\n",
    "    plt.show()\n",
    "\n",
    "    # Pause and wait for user input\n",
    "    input(\"Press Enter to continue...\")\n",
    "    print(f'Channels to be excluded: {ica.exclude}')\n",
    "\n",
    "    #%%############################################################################\n",
    "    \"\"\"Channel selection - based on van Ede et al. 2011\"\"\"\n",
    "    ###############################################################################\n",
    "    if Chan_sel:\n",
    "        print('Starting channel selection....')\n",
    "        # plot erp\n",
    "        # Selection_epochs[\"Left\"].average().plot(picks = ['Cz','C3','C4'])\n",
    "        # Selection_epochs[\"Right\"].apply_baseline((None,0)).average().plot_topomap(times= np.linspace(0.1, 0.5, 5))\n",
    "        \n",
    "        left_chans = ['FC1', 'C3', 'CP1']\n",
    "        right_chans = ['FC2', 'C4', 'CP2']\n",
    "\n",
    "        freqs = np.linspace(1,30,30)\n",
    "        freqs = np.array([1, 30]) # for stockwell method\n",
    "        power = Selection_epochs.compute_tfr(method = 'morlet', \n",
    "                                            freqs = freqs,\n",
    "                                            n_cycles = freqs/2,\n",
    "                                            output='power',\n",
    "                                            average=False, \n",
    "                                            return_itc=False, \n",
    "                                            decim=2)\n",
    "\n",
    "\n",
    "        # power.save(os.path.join(working_dir + 'Processed/', file[:6] + 'Chan_Sel-tfr.hdf5'), overwrite = True)\n",
    "        left = power['Left'].average().crop(tmin = .1, tmax = .3, fmin = 15, fmax = 30).get_data()\n",
    "        right = power['Right'].average().crop(tmin = .1, tmax = .3, fmin = 15, fmax = 30).get_data()\n",
    "\n",
    "        lmean = left.mean(axis=(1, 2))\n",
    "        rmean = right.mean(axis=(1, 2))\n",
    "        result = (rmean - lmean) / (rmean + lmean)\n",
    "\n",
    "        # Get indices of the three largest values\n",
    "        indices_of_largest = np.argsort(result)[-3:]\n",
    "        indices_of_smallest = np.argsort(result)[:3]\n",
    "\n",
    "        # Get the channel names corresponding to these indices\n",
    "        channel_names_of_largest = [power.info['ch_names'][i] for i in indices_of_largest]\n",
    "        channel_names_of_smallest = [power.info['ch_names'][i] for i in indices_of_smallest]\n",
    "        print(f'Channels with largest difference: {channel_names_of_largest}')\n",
    "        print(f'Channels with smallest difference: {channel_names_of_smallest}')\n",
    "\n",
    "        left_chans = channel_names_of_largest # Check if this is correct\n",
    "        right_chans = channel_names_of_smallest # Check if this is correct\n",
    "\n",
    "    # Apply ICA solution to the same epochs\n",
    "    ica.apply(Selection_epochs)\n",
    "\n",
    "    # Create folder for this participant in the processed folder\n",
    "    Write_path = os.path.join(working_dir + 'Processed/', file[:6] + '/')\n",
    "    if not os.path.exists(Write_path):\n",
    "        os.makedirs(Write_path)\n",
    "        print(f\"Subfolder created at: {Write_path}\")\n",
    "    else:\n",
    "        print(f\"Subfolder already exists at: {Write_path}\")\n",
    "\n",
    "    epo_fName = os.path.join(Write_path + file[:6] + '_ERPs-epo.fif')\n",
    "    Selection_epochs.save(epo_fName, overwrite = True)\n",
    "    with open(Write_path + file[:6] + 'bad_channels_ERP.txt', 'w') as f:\n",
    "        for channel in bad_channels_ERPs:\n",
    "            f.write(f\"{channel}\\n\")\n",
    "\n",
    "\n",
    "    Selection_epochs.apply_baseline((None,0))\n",
    "    freqs = np.arange(.3, 35, 1)\n",
    "    n_cycles = freqs / 2.0\n",
    "    power = Selection_epochs.compute_tfr(method = 'morlet',\n",
    "                                            freqs = freqs,\n",
    "                                            n_cycles = n_cycles,\n",
    "                                            output='power',\n",
    "                                            average=False,\n",
    "                                            return_itc=False,\n",
    "                                            decim=1)\n",
    "\n",
    "    power.save(os.path.join(Write_path + file[:6] + 'ERPs-tfr.hdf5'), overwrite = True)\n",
    "\n",
    "    # Save the ICA soulution that will be applied to the data\n",
    "    compname = os.path.join(Write_path + file[:6] + '_weights-ica.fif')\n",
    "    ica.save(fname = compname, overwrite = True)\n",
    "\n",
    "    del Selection_epochs, data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%############################################################################\n",
    "\"\"\"import data\"\"\"\n",
    "\"\"\"This code is only for performing the BBO preprocessing, not the ERP part\"\"\"\n",
    "###############################################################################\n",
    "working_dir = data_path + \"EEG/\"\n",
    "EEGfiles = [f for f in os.listdir(working_dir) if f.endswith('.vhdr')]\n",
    "\n",
    "for sub in range(0,len(EEGfiles)):\n",
    "    index = sub\n",
    "    file = EEGfiles[index]\n",
    "    print(f'Working on file {file}')\n",
    "    EEG_fpath = os.path.join(working_dir, file)\n",
    "\n",
    "    ###############################################################################\n",
    "    \"\"\"Import the original data for BBO\"\"\"\n",
    "    ###############################################################################\n",
    "    BBO = mne.io.read_raw_brainvision(EEG_fpath, preload = True)\n",
    "    # Filter data\n",
    "    BBO.load_data().filter(l_freq = 3, h_freq = 50)\n",
    "    BBO.notch_filter(freqs=(50,100), filter_length='auto', notch_widths = 3)\n",
    "    BBO.set_eeg_reference(ref_channels = \"average\")\n",
    "\n",
    "    events = mne.events_from_annotations(BBO)\n",
    "    events = events[0]\n",
    "    event_id = {'Left':10002,'Invalid':10003, 'Right':10004}\n",
    "\n",
    "    # Remove invalid events\n",
    "    event_dict = {}\n",
    "    eventcount = {}\n",
    "    for key, val in event_id.items():\n",
    "        numevents = len(events[events[:,2] == val])\n",
    "        temp = events[events[:,2] == val]\n",
    "        print(f'The original number of events for {key} is {numevents}')\n",
    "        eventcount[key] = numevents\n",
    "        event_dict[key] = temp\n",
    "        if numevents > 45:\n",
    "            temp = temp[-45:,:]\n",
    "            event_dict[key] = temp\n",
    "        elif numevents == 0:\n",
    "            temp = []\n",
    "            event_dict[key] = temp\n",
    "        elif numevents == 12:\n",
    "            event_dict[key] = temp\n",
    "\n",
    "    # Set Montage\n",
    "    montage = mne.channels.make_standard_montage('easycap-M1')\n",
    "    BBO.rename_channels({'M1': 'TP9', 'M2': 'TP10'})\n",
    "    BBO.set_montage(montage)\n",
    "\n",
    "    ###############################################################################\n",
    "    \"\"\"Import the csv file with psychopy data\"\"\"\n",
    "    ###############################################################################\n",
    "\n",
    "     # Import the csv file with psychopy data\n",
    "    responsepath = data_path + \"Psychopy/\"\n",
    "    # extract all .csv files in the folder\n",
    "    responsefiles = [f for f in os.listdir(responsepath) if f.endswith('.csv') and (f.startswith(str(index+1)+'_') and not f.__contains__('trial'))]\n",
    "    # # sort in ascending order\n",
    "    responsefiles.sort()\n",
    "\n",
    "    # try each file to see if it is the correct one\n",
    "    for filename in responsefiles:\n",
    "        fpath = os.path.join(responsepath, filename)\n",
    "        responses = pd.read_csv(fpath)\n",
    "        if 'p_port' not in responses.keys():\n",
    "            continue\n",
    "        elif len(responses['p_port']>1) == 102:\n",
    "            break\n",
    "        \n",
    "    # Assert that the EEG and the response file match in the participant number\n",
    "    assert int(filename.split('_')[0]) == index+1 == int(file.split('_')[1]), 'EEG and Psychopy files dont match'\n",
    "\n",
    "    # If there are no right events, infer the right indices and correct for drift\n",
    "    if len(event_dict['Right']) == 0:\n",
    "        print('No right events found. Inferring and correcting for drift...')\n",
    "        srate = BBO.info['sfreq']\n",
    "        left_idx = event_dict['Left'][:,0]\n",
    "        right_idx = infer_rights(responses, srate, left_idx)\n",
    "        right_idx = correct_drift(responses, srate, left_idx, right_idx)\n",
    "        event_dict['Right'] = np.zeros((len(right_idx),3))\n",
    "        event_dict['Right'][:,0] = right_idx\n",
    "        event_dict['Right'][:,1] = 0\n",
    "        event_dict['Right'][:,2] = 10004\n",
    "\n",
    "    # Save the events in the EEG file\n",
    "    events_in_eeg = np.load(os.path.join(working_dir,'events_in_eeg.npy'), allow_pickle=True).item()\n",
    "    events_in_eeg[file] = eventcount # This will not work\n",
    "    np.save(os.path.join(working_dir,'events_in_eeg.npy'), events_in_eeg) \n",
    "\n",
    "    # Now convert the dict back to an array\n",
    "    events = np.concatenate([v for v in event_dict.values()], axis=0)\n",
    "    del event_dict\n",
    "    events = events.astype(int)\n",
    "    events = events[events[:,0].argsort()]\n",
    "\n",
    "    assert len(events[:,2][events[:,2] == 10002]) == 45 == len(events[:,2][events[:,2] == 10004]), 'Events are not 45'\n",
    "    assert len(events[:,2][events[:,2] == 10003]) == 12, 'Invalid events are not 12'\n",
    "\n",
    "    ###############################################################################\n",
    "    \"\"\"remove and interpolate bad channels\"\"\"\n",
    "    ###############################################################################\n",
    "\n",
    "    # Instantiate NoisyChannels with Raw data\n",
    "    noisy_chans = ppr.NoisyChannels(BBO, do_detrend = False)\n",
    "\n",
    "    # Find bad by correlation\n",
    "    noisy_chans.find_bad_by_correlation(correlation_secs=5.0, correlation_threshold=0.4, frac_bad=0.1)\n",
    "    print(f'After correlation removal: {noisy_chans.get_bads()}')\n",
    "    BBO.info['bads'].extend(noisy_chans.get_bads())  # Update bads\n",
    "\n",
    "    # Find bad by deviation\n",
    "    noisy_chans.find_bad_by_deviation()\n",
    "    print(f'After deviation removal: {noisy_chans.bad_by_deviation}')\n",
    "    BBO.info['bads'].extend(noisy_chans.get_bads())  # Update bads\n",
    "\n",
    "    # Find bad by high-frequency noise\n",
    "    noisy_chans.find_bad_by_hfnoise(HF_zscore_threshold=5.0)\n",
    "    print(f'After HF noise removal: {noisy_chans.get_bads()}')\n",
    "    BBO.info['bads'].extend(noisy_chans.get_bads())  # Update bads\n",
    "    bad_channels_Final = list(set(BBO.info['bads']))\n",
    "    BBO.interpolate_bads(reset_bads=True)\n",
    "\n",
    "    #%%############################################################################\n",
    "    \"\"\"Epoch data\"\"\"\n",
    "    ###############################################################################\n",
    "\n",
    "    epstart = -4\n",
    "    epend = 1\n",
    "    real_epochs = mne.Epochs(BBO,\n",
    "                        events = events,\n",
    "                        event_id = event_id,\n",
    "                        tmin = epstart, tmax = epend,\n",
    "                        proj = False, baseline = None,\n",
    "                        preload = True, reject = None)\n",
    "    real_epochs.set_eeg_reference(ref_channels = \"average\")\n",
    "    real_epochs.apply_baseline(baseline=(None, None))\n",
    "\n",
    "    # Create folder for this participant in the processed folder\n",
    "    Write_path = os.path.join(working_dir + 'Processed/', file[:6] + '/')\n",
    "    if not os.path.exists(Write_path):\n",
    "        os.makedirs(Write_path)\n",
    "        print(f\"Subfolder created at: {Write_path}\")\n",
    "    else:\n",
    "        print(f\"Subfolder already exists at: {Write_path}\")\n",
    "\n",
    "    # save the final version of the data\n",
    "    epo_fName = os.path.join(Write_path + file[:6] + '_for_BBO-epo.fif')\n",
    "    real_epochs.save(epo_fName, overwrite = True)\n",
    "    with open(Write_path + file[:6] + 'bad_channels_Final.txt', 'w') as f:\n",
    "        for channel in bad_channels_Final:\n",
    "            f.write(f\"{channel}\\n\")\n",
    "\n",
    "    freqs = np.arange(3, 51, 1)\n",
    "    n_cycles = freqs / 2.0\n",
    "    power = real_epochs.compute_tfr(method = 'morlet',\n",
    "                                            freqs = freqs,\n",
    "                                            n_cycles = n_cycles,\n",
    "                                            output='power',\n",
    "                                            average=False,\n",
    "                                            return_itc=False,\n",
    "                                            decim=1)\n",
    "\n",
    "    power.save(os.path.join(Write_path + file[:6] + '-tfr.hdf5'), overwrite = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
