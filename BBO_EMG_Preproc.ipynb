{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook performs all the EMG processing, primarily using MNE and other custom functions for artefact correction using a template matching approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "from mne import create_info\n",
    "from mne.io import RawArray\n",
    "import pickle\n",
    "from scipy.signal import firwin, lfilter\n",
    "from scipy.fftpack import fft\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.stats import linregress\n",
    "import scipy.signal\n",
    "from BBO_Analysis_Functions import infer_rights, correct_drift, find_correlation_peaks, extract_correlations, correct_data_with_template, correct_data_with_template_2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File index to extract\n",
    "EMGpath = \"Path to EMG data/EMG/\" \n",
    "# extract all .mat files in the folder\n",
    "EMGfiles = [f for f in os.listdir(EMGpath) if f.endswith('.mat')]\n",
    "for EMGfile in EMGfiles:\n",
    "    # if the file name contains test, remove it\n",
    "    if 'test' in EMGfile:\n",
    "        print(EMGfile)\n",
    "        EMGfiles.remove(EMGfile)\n",
    "\n",
    "\n",
    "for index in range(len(EMGfiles)):\n",
    "    EMGfile = EMGfiles[index]\n",
    "    participant = EMGfile[:-4]\n",
    "    fpath = os.path.join(EMGpath, EMGfile)\n",
    "    data = scipy.io.loadmat(fpath)\n",
    "    keys = list(data.keys())\n",
    "    print(participant)\n",
    "    srate = 2000\n",
    "\n",
    "    processed_path = \"Path to EMG data/Processed/\"\n",
    "    if not os.path.exists(processed_path):\n",
    "        os.makedirs(processed_path)\n",
    "        print(f\"Folder created: {processed_path}\")\n",
    "    else:\n",
    "        print(f\"Folder already exists: {processed_path}\")\n",
    "\n",
    "    subject_write_path = os.path.join(processed_path, participant)\n",
    "    if not os.path.exists(subject_write_path):\n",
    "        os.makedirs(subject_write_path)\n",
    "        print(f\"Folder created: {subject_write_path}\")\n",
    "\n",
    "    # Import the csv file with psychopy data\n",
    "    responsepath = \"Path to Psychopy data/Psychopy/\"\n",
    "    responsefiles = [f for f in os.listdir(responsepath) if f.endswith('.csv') and (f.__contains__(EMGfile[3:-4] + '_' + EMGfile[:3]) and f.startswith(EMGfile[3:-4]+'_') and not f.__contains__('trial'))]\n",
    "    responsefiles.sort()\n",
    "    filename = responsefiles[0]\n",
    "    print(filename)\n",
    "\n",
    "    # try each file to see if it is the correct one\n",
    "    for filename in responsefiles:\n",
    "        fpath = os.path.join(responsepath, filename)\n",
    "        responses = pd.read_csv(fpath)\n",
    "        if 'p_port' not in responses.keys():\n",
    "            continue\n",
    "        elif len(responses['p_port']>1) == 102:\n",
    "            break\n",
    "        \n",
    "    # Assert that the EEG and the response file match in the participant number\n",
    "    assert int(filename.split('_')[0]) == index+1 == int(participant[3:]), 'EMG and Psychopy files do not match'\n",
    "\n",
    "    # Store behavioural data\n",
    "    age = responses['O_Age.response'].dropna().values[0]\n",
    "    gender = responses['slider_gender.response'].dropna().values[0]\n",
    "    participant_response = responses['response_participant.keys'][responses['p_port'] > 1].values\n",
    "    print(len(participant_response))\n",
    "    matches = responses['response_participant.corr'][responses['p_port'] > 1].values\n",
    "    rts = responses['response_participant.rt'][responses['p_port'] > 1].values\n",
    "    correct_resp = responses['correct_resp'][responses['p_port'] > 1].values\n",
    "    intensities = responses['intensity_rating.response'].dropna().values\n",
    "    cues = responses['cue'][responses['p_port'] > 1].values\n",
    "    validity = responses['validity'][responses['p_port'] > 1].values\n",
    "\n",
    "    behavioural_data = {\n",
    "        'age': age,\n",
    "        'gender': gender,\n",
    "        'participant_response': participant_response,\n",
    "        'matches': matches,\n",
    "        'rts': rts,\n",
    "        'correct_resp': correct_resp,\n",
    "        'intensities': intensities,\n",
    "        'cues': cues,\n",
    "        'validity': validity\n",
    "    }\n",
    "\n",
    "    # Extract the data and the stimulus channels\n",
    "    left_signal = data['data'][:,0]/1000\n",
    "    right_signal = data['data'][:,1]/1000\n",
    "    left_stim = data['data'][:,2]\n",
    "    invalid_stim = data['data'][:,3]\n",
    "    right_stim = data['data'][:,4]\n",
    "\n",
    "    # Delete the data variable\n",
    "    del data\n",
    "    timevec = np.arange(0, len(left_signal)/srate, 1/srate)\n",
    "    def get_indices(stim):\n",
    "        stim_idx = np.where(np.diff(stim) > 2)[0]\n",
    "        num_trig = len(stim_idx)\n",
    "        if len(stim_idx)>45:\n",
    "            print(f'The original number of stim indices is {len(stim_idx)}')\n",
    "            stim_idx = stim_idx[-45:]\n",
    "        return stim_idx, num_trig\n",
    "\n",
    "    # Stim indices\n",
    "    left_idx, numleft = get_indices(left_stim)\n",
    "    right_idx, numright = get_indices(right_stim)\n",
    "    invalid_idx, numinvalid = get_indices(invalid_stim)\n",
    "    behavioural_data['left_idx'] = left_idx\n",
    "    behavioural_data['right_idx'] = right_idx\n",
    "    behavioural_data['invalid_idx'] = invalid_idx\n",
    "\n",
    "    Orig_eventcount = {'left': numleft, 'right': numright, 'invalid': numinvalid}\n",
    "    print(f'Left markers: {len(left_idx)}, Right markers: {len(right_idx)}, Invalid markers: {len(invalid_idx)}')\n",
    "    if not os.path.exists(os.path.join(EMGpath,'events_in_emg.npy')):\n",
    "        events_in_emg = {}\n",
    "    elif os.path.exists(os.path.join(EMGpath,'events_in_emg.npy')):\n",
    "        events_in_emg = np.load(os.path.join(EMGpath,'events_in_emg.npy'), allow_pickle=True).item()\n",
    "    events_in_emg[EMGfile] = Orig_eventcount # This will not work\n",
    "    np.save(os.path.join(EMGpath,'events_in_emg.npy'), events_in_emg) \n",
    "    del events_in_emg\n",
    "\n",
    "    # Fix the markers\n",
    "    if len(right_idx) < 45:\n",
    "        print('Right markers are missing')\n",
    "        right_idx = infer_rights(responses, srate, left_idx)\n",
    "        print('Fixing right markers')\n",
    "        right_idx = correct_drift(responses, srate, left_idx, right_idx)\n",
    "        print(f'Corrected Right markers: {len(right_idx)}')\n",
    "\n",
    "    if len(invalid_idx) > 12:\n",
    "        invalid_idx = invalid_idx[-12:]\n",
    "\n",
    "    assert len(right_idx) == 45, 'Right markers are still missing'\n",
    "    assert len(left_idx) == 45, 'Left markers are still missing'\n",
    "    assert len(invalid_idx) == 12, 'Invalid markers are still missing'\n",
    "\n",
    "    # how many lefts and rights\n",
    "    print('Number of left cues: ', len(cues[(cues == 'left') & (validity == 'valid')]))\n",
    "    print('Number of right cues: ', len(cues[(cues == 'right') & (validity == 'valid')]))\n",
    "    print('Number of invalid left cues: ', len(cues[(cues == 'left') & (validity == 'invalid')]))\n",
    "    print('Number of invalid right cues: ', len(cues[(cues == 'right') & (validity == 'invalid')]))\n",
    "    print('First cue: ', cues[0], '; First validity: ', validity[0])\n",
    "\n",
    "    assert len(left_signal) == len(right_signal), \"Left and right signals dont have the same length.\"\n",
    "\n",
    "    emg_data = np.vstack([left_signal, right_signal])\n",
    "    print(f'EMG data shape: {emg_data.shape}')\n",
    "    ch_names = ['EMG_left', 'EMG_right']  # Channel names\n",
    "    ch_types = ['emg', 'emg']  # Channel types\n",
    "    info = create_info(ch_names=ch_names, sfreq=srate, ch_types=ch_types)\n",
    "    raw_EMG = RawArray(emg_data, info)\n",
    "    raw_EMG = raw_EMG.resample(1000)\n",
    "    info = raw_EMG.info\n",
    "\n",
    "    # Filter the EMG data\n",
    "    raw_EMG.filter(l_freq=3, h_freq=50, picks = ch_names,fir_design='firwin')\n",
    "    raw_EMG.notch_filter(freqs=(50,100), filter_length='auto', picks = ch_names, notch_widths = 3)\n",
    "\n",
    "    ############################## Clean data with template matching ############################## \n",
    "    raw_data = raw_EMG.get_data(picks = 'emg')\n",
    "    template_28 = np.load('Path to EMG data/Processed/BBO28/BBO28_EMG_template.npy')\n",
    "    # Normalize the template before convolution\n",
    "    norm_template_28 = template_28.mean(axis = 0)\n",
    "    norm_template_28 /= np.linalg.norm(norm_template_28)\n",
    "\n",
    "    All_peaks = find_correlation_peaks(raw_data,norm_template_28)\n",
    "    All_correlations = extract_correlations(raw_data, norm_template_28, All_peaks)\n",
    "    Corrected_data, All_errors = correct_data_with_template(\n",
    "        raw_data=raw_data,\n",
    "        norm_template_28=norm_template_28,\n",
    "        All_peaks=All_peaks,\n",
    "        corrthresh=0.8\n",
    "    )\n",
    "\n",
    "    # Now use the second template\n",
    "    template_2 = np.load('Path to EMG data/Processed/BBO13/template_2.npy')\n",
    "    template_2 = template_2.mean(axis = 0)\n",
    "    template_2 /= np.linalg.norm(template_2)\n",
    "    All_peaks = find_correlation_peaks(Corrected_data,template_2)\n",
    "    All_correlations = extract_correlations(Corrected_data, template_2, All_peaks)\n",
    "    Corrected_data_2, All_errors_2, subtracted_segments = correct_data_with_template_2(\n",
    "        Input_data=Corrected_data,\n",
    "        template_2=template_2,\n",
    "        All_peaks=All_peaks,\n",
    "        corrthresh=0.8\n",
    "    )\n",
    "\n",
    "    Corrected_EMG = RawArray(Corrected_data_2, info)\n",
    "    # del Corrected_data, All_errors, All_peaks, All_correlations\n",
    "\n",
    "    # Define Events\n",
    "    events = np.zeros((len(left_idx) + len(right_idx) + len(invalid_idx), 3))\n",
    "    events[:len(left_idx), 0] = left_idx\n",
    "    events[:len(left_idx), 2] = 1\n",
    "    events[len(left_idx):len(left_idx) + len(right_idx), 0] = right_idx\n",
    "    events[len(left_idx):len(left_idx) + len(right_idx), 2] = 2\n",
    "    events[len(left_idx) + len(right_idx):, 0] = invalid_idx\n",
    "    events[len(left_idx) + len(right_idx):, 2] = 3\n",
    "    events[:, 0] = (events[:, 0] / 2).round() # downsample the events\n",
    "    events = events.astype(int)\n",
    "    events = events[events[:, 0].argsort()]\n",
    "    event_id = {'Left': 1, 'Invalid':3, 'Right':2}\n",
    "\n",
    "    tmin = -4\n",
    "    tmax = 0\n",
    "    epochs_EMG = mne.Epochs(Corrected_EMG, events, event_id, tmin, tmax, baseline=None, preload=True)\n",
    "    epochs_EMG.apply_baseline(baseline=(None, None))\n",
    "\n",
    "    # Time-frequency\n",
    "    frequencies = np.arange(3, 51, 1)  # Define frequencies of interest\n",
    "    n_cycles = frequencies/2\n",
    "    EMG_tfr =  epochs_EMG.compute_tfr(method = 'morlet', \n",
    "                                        freqs = frequencies,\n",
    "                                        n_cycles = n_cycles,\n",
    "                                        output='power',\n",
    "                                        picks='emg',\n",
    "                                        average=False, \n",
    "                                        return_itc=False)\n",
    "                                    \n",
    "    # Save the extracted behavioural data \n",
    "    file_path = os.path.join(subject_write_path, f'{participant}_behavioural_data.pkl')\n",
    "    with open(file_path, 'wb') as pickle_file:\n",
    "        pickle.dump(behavioural_data, pickle_file)\n",
    "\n",
    "    # Save the processed EMG data\n",
    "    Processed_epochs_filename = os.path.join(subject_write_path, f'{participant}_EMG-epo.fif')\n",
    "    epochs_EMG.save(Processed_epochs_filename, overwrite=True)\n",
    "    print(f'File saved: epochs_EMG')\n",
    "\n",
    "    # Save the processed EMG tfr data\n",
    "    Processed_tfr_filename = os.path.join(subject_write_path, f'{participant}_EMG-tfr.h5')\n",
    "    EMG_tfr.save(Processed_tfr_filename, overwrite=True)\n",
    "    print(f'File saved: EMG_tfr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the pickle file\n",
    "# with open(file_path, 'rb') as pickle_file:\n",
    "#     loaded_data = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[264 802 263 533 266 534 266]\n"
     ]
    }
   ],
   "source": [
    "# Code for template extraction\n",
    "trial_idx = 53\n",
    "this_trial = sample_data[trial_idx,0,:]\n",
    "thresh = this_trial.std()*3\n",
    "half_len = int(264/2)\n",
    "second_half_len = int(264/2) + 100\n",
    "# Find the peaks\n",
    "peaks = scipy.signal.find_peaks(this_trial, height = thresh)\n",
    "plt.plot(this_trial)\n",
    "plt.plot(peaks[0], this_trial[peaks[0]], 'ro')\n",
    "plt.axhline(thresh, color = 'r')\n",
    "plt.show()\n",
    "print(np.diff(peaks[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each peak, if the peak is not at the edge of the epoch, extract the data around the peak\n",
    "if 'template2' not in globals():\n",
    "    template2 = []\n",
    "for peak in peaks[0]:\n",
    "    if peak > half_len and peak < len(this_trial) - second_half_len:\n",
    "        template2.append(this_trial[peak-half_len:peak+second_half_len])\n",
    "print(len(template2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template_2 = np.load('Path to EMG data/Processed/BBO13/template_2.npy')\n",
    "template_2 = template_2.mean(axis = 0)\n",
    "template_2 /= np.linalg.norm(template_2)\n",
    "Input_data = Corrected_data\n",
    "All_peaks = find_correlation_peaks(Corrected_data,template_2)\n",
    "corrthresh = 0.7\n",
    "smooth = False\n",
    "template_length = len(template_2)\n",
    "Corrected_data_2 = np.copy(Input_data)\n",
    "if smooth:\n",
    "    from scipy.ndimage import gaussian_filter1d\n",
    "    template_2 = gaussian_filter1d(template_2, sigma=6)\n",
    "# Taper the edges of the template\n",
    "negative_ramp = [0, 100]\n",
    "positive_ramp = [template_length - 100, template_length]\n",
    "ramp_up = np.hanning((negative_ramp[1] - negative_ramp[0]) * 2)[:(negative_ramp[1] - negative_ramp[0])]\n",
    "ramp_down = np.hanning((positive_ramp[1] - positive_ramp[0]) * 2)[(positive_ramp[1] - positive_ramp[0]):]\n",
    "tapered_template = np.copy(template_2)\n",
    "tapered_template[:negative_ramp[1]] *= ramp_up\n",
    "tapered_template[positive_ramp[0]:] *= ramp_down\n",
    "subtracted_segments = []\n",
    "# Define the windows for scaling\n",
    "negative_win = [187, 217] \n",
    "positive_win = [120, 150]\n",
    "\n",
    "# Initialize error dictionary\n",
    "All_errors = {'left': np.zeros(len(All_peaks[0])), 'right': np.zeros(len(All_peaks[1]))}\n",
    "subtracted_segments = {\n",
    "'left': np.zeros((len(All_peaks[0]), template_length)), \n",
    "'right': np.zeros((len(All_peaks[1]), template_length))}\n",
    "Scaled_templates = {\n",
    "    'left': np.zeros((len(All_peaks[0]), template_length)),\n",
    "    'right': np.zeros((len(All_peaks[1]), template_length))\n",
    "}\n",
    "# Process each channel and peak\n",
    "for channel in range(Corrected_data_2.shape[0]):\n",
    "    for idx, peak in enumerate(All_peaks[channel]):\n",
    "        tempsegment = Corrected_data_2[channel, peak - (template_length // 2):peak + (template_length // 2)]\n",
    "        if len(tempsegment) != template_length:\n",
    "            continue\n",
    "\n",
    "        # Check if it is a match\n",
    "        correlation = np.corrcoef(tempsegment, template_2)[0, 1]\n",
    "        if correlation > corrthresh:\n",
    "            scaling_window = [positive_win[0], negative_win[1]]\n",
    "            slope, intercept, _, _, _ = linregress(\n",
    "                tapered_template[scaling_window[0]:scaling_window[1]],\n",
    "                tempsegment[scaling_window[0]:scaling_window[1]]\n",
    "            )\n",
    "            scaled_template = tapered_template * slope + intercept\n",
    "\n",
    "            # Further scale each peak individually\n",
    "            scaled_template[positive_win[0]:positive_win[1]] *= (\n",
    "                tempsegment[positive_win[0]:positive_win[1]].max() / scaled_template.max()\n",
    "            )\n",
    "            scaled_template[negative_win[0]:negative_win[1]] *= (\n",
    "                tempsegment[negative_win[0]:negative_win[1]].min() / scaled_template.min()\n",
    "            )\n",
    "\n",
    "            # Subtract the scaled template\n",
    "            subtracted_seg = tempsegment - scaled_template\n",
    "\n",
    "            # Calculate squared error\n",
    "            squared_residuals = (subtracted_seg) ** 2\n",
    "            error = np.sum(squared_residuals)\n",
    "            if channel == 0:\n",
    "                All_errors['left'][idx] = error\n",
    "                subtracted_segments['left'][idx] = subtracted_seg\n",
    "                Scaled_templates['left'][idx] = scaled_template\n",
    "            elif channel == 1:\n",
    "                All_errors['right'][idx] = error\n",
    "                subtracted_segments['right'][idx] = subtracted_seg\n",
    "                Scaled_templates['right'][idx] = scaled_template\n",
    "\n",
    "            # Update the corrected data\n",
    "            Corrected_data_2[channel, peak - (template_length // 2):peak + (template_length // 2)] = subtracted_seg\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
